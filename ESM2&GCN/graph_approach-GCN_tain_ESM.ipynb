{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb4734a-6a8f-49fd-91c9-4c2e5676213a",
   "metadata": {},
   "source": [
    "# In this notebook we will benifit from the new features embeeding to train GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d514801b-cb8f-49c6-85c5-84a49a687ee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:04:26.372679Z",
     "iopub.status.busy": "2023-01-22T21:04:26.372403Z",
     "iopub.status.idle": "2023-01-22T21:04:33.621942Z",
     "shell.execute_reply": "2023-01-22T21:04:33.621084Z",
     "shell.execute_reply.started": "2023-01-22T21:04:26.372658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# !pip install tensorflow\n",
    "# !pip install  spacy\n",
    "# !pip install tqdm\n",
    "# !pip install plotly\n",
    "!pip install jupyter-black\n",
    "!pip install imblearn\n",
    "!pip install joblib --upgrade\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab47875-d7b8-467e-9f9a-824abb3b9105",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92068b-ba2c-4f7d-b774-e77d9743c4f9",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3f4560-096a-4342-9371-ccb1bbaef8b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:05:17.195107Z",
     "iopub.status.busy": "2023-01-22T21:05:17.194422Z",
     "iopub.status.idle": "2023-01-22T21:05:17.204117Z",
     "shell.execute_reply": "2023-01-22T21:05:17.203621Z",
     "shell.execute_reply.started": "2023-01-22T21:05:17.195083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5fcdd3-4daf-4942-bdec-989e2c169e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:05:18.131151Z",
     "iopub.status.busy": "2023-01-22T21:05:18.130625Z",
     "iopub.status.idle": "2023-01-22T21:05:18.136199Z",
     "shell.execute_reply": "2023-01-22T21:05:18.135714Z",
     "shell.execute_reply.started": "2023-01-22T21:05:18.131129Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a331634-a658-4862-acd5-cb498ef7064f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:05:18.917423Z",
     "iopub.status.busy": "2023-01-22T21:05:18.916882Z",
     "iopub.status.idle": "2023-01-22T21:05:18.921153Z",
     "shell.execute_reply": "2023-01-22T21:05:18.920680Z",
     "shell.execute_reply.started": "2023-01-22T21:05:18.917400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1968a-5a81-4861-a099-c1346ec96ddc",
   "metadata": {},
   "source": [
    "### Important functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a916f1a-317a-402a-9ec6-db0923a97dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:05:21.847633Z",
     "iopub.status.busy": "2023-01-22T21:05:21.847111Z",
     "iopub.status.idle": "2023-01-22T21:05:21.854231Z",
     "shell.execute_reply": "2023-01-22T21:05:21.853782Z",
     "shell.execute_reply.started": "2023-01-22T21:05:21.847610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    f_x = y / np.sum(np.exp(x))\n",
    "    return f_x\n",
    "\n",
    "\n",
    "softmax_vect = np.vectorize(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8af066c-582f-4792-abaa-5f32e32f2ff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:05:23.059940Z",
     "iopub.status.busy": "2023-01-22T21:05:23.059274Z",
     "iopub.status.idle": "2023-01-22T21:05:23.067893Z",
     "shell.execute_reply": "2023-01-22T21:05:23.067414Z",
     "shell.execute_reply.started": "2023-01-22T21:05:23.059917Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weighted multi-class log loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def weighted_mc_log_loss(y_true, y_pred, y_pred_proba):\n",
    "    loss = log_loss(y_true, y_pred_proba, labels=np.unique(y_true))\n",
    "    accuracy = round((y_true == y_pred).sum() / len(y_true) * 100, 2)\n",
    "    return print(f\"{loss = } and accuracy {accuracy = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00dc7a6f-c7d5-4cbb-b0a7-539e42eeeb2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:05:24.193359Z",
     "iopub.status.busy": "2023-01-22T21:05:24.192849Z",
     "iopub.status.idle": "2023-01-22T21:05:24.206556Z",
     "shell.execute_reply": "2023-01-22T21:05:24.206031Z",
     "shell.execute_reply.started": "2023-01-22T21:05:24.193336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submit(y_pred_proba, name=\"\"):\n",
    "    # Write predictions to a file\n",
    "    with open(\"../Submissions/\" + name + \"Graph_gcn.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        lst = list()\n",
    "        for i in range(18):\n",
    "            lst.append(\"class\" + str(i))\n",
    "        lst.insert(0, \"name\")\n",
    "        writer.writerow(lst)\n",
    "        for i, protein in enumerate(proteins_test):\n",
    "            lst = y_pred_proba[i, :].tolist()\n",
    "            lst.insert(0, protein)\n",
    "            writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d628810-a306-488c-b82b-cf58036532fa",
   "metadata": {},
   "source": [
    "# using structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be017f7-0fea-45fd-ae55-1f00d2c60aa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:05:25.958621Z",
     "iopub.status.busy": "2023-01-22T21:05:25.957953Z",
     "iopub.status.idle": "2023-01-22T21:05:26.484157Z",
     "shell.execute_reply": "2023-01-22T21:05:26.483586Z",
     "shell.execute_reply.started": "2023-01-22T21:05:25.958596Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc2aae1-32bf-4772-8bb2-b001267ecc0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:06:05.700247Z",
     "iopub.status.busy": "2023-01-22T21:06:05.699676Z",
     "iopub.status.idle": "2023-01-22T21:06:05.732872Z",
     "shell.execute_reply": "2023-01-22T21:06:05.732221Z",
     "shell.execute_reply.started": "2023-01-22T21:06:05.700225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Function that loads graphs\n",
    "    \"\"\"\n",
    "    graph_indicator = np.loadtxt(path + \"graph_indicator.txt\", dtype=np.int64)\n",
    "    _, graph_size = np.unique(graph_indicator, return_counts=True)\n",
    "\n",
    "    edges = np.loadtxt(path + \"edgelist.txt\", dtype=np.int64, delimiter=\",\")\n",
    "    edges_inv = np.vstack((edges[:, 1], edges[:, 0]))\n",
    "    edges = np.vstack((edges, edges_inv.T))\n",
    "    s = edges[:, 0] * graph_indicator.size + edges[:, 1]\n",
    "    idx_sort = np.argsort(s)\n",
    "    edges = edges[idx_sort, :]\n",
    "    edges, idx_unique = np.unique(edges, axis=0, return_index=True)\n",
    "    A = sp.csr_matrix(\n",
    "        (np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "        shape=(graph_indicator.size, graph_indicator.size),\n",
    "    )\n",
    "\n",
    "    x = np.loadtxt(path + \"node_attributes.txt\", delimiter=\",\")\n",
    "    edge_attr = np.loadtxt(path + \"edge_attributes.txt\", delimiter=\",\")\n",
    "    edge_attr = np.vstack((edge_attr, edge_attr))\n",
    "    edge_attr = edge_attr[idx_sort, :]\n",
    "    edge_attr = edge_attr[idx_unique, :]\n",
    "\n",
    "    adj = []\n",
    "    features = []\n",
    "    edge_features = []\n",
    "    idx_n = 0\n",
    "    idx_m = 0\n",
    "    for i in range(graph_size.size):\n",
    "        adj.append(A[idx_n : idx_n + graph_size[i], idx_n : idx_n + graph_size[i]])\n",
    "        edge_features.append(edge_attr[idx_m : idx_m + adj[i].nnz, :])\n",
    "        features.append(x[idx_n : idx_n + graph_size[i], :])\n",
    "        idx_n += graph_size[i]\n",
    "        idx_m += adj[i].nnz\n",
    "\n",
    "    return adj, features, edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a06e3c3-1719-45bc-b17d-57424bbe290b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:06:05.938752Z",
     "iopub.status.busy": "2023-01-22T21:06:05.938218Z",
     "iopub.status.idle": "2023-01-22T21:06:05.953902Z",
     "shell.execute_reply": "2023-01-22T21:06:05.953323Z",
     "shell.execute_reply.started": "2023-01-22T21:06:05.938731Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    \"\"\"\n",
    "    Function that normalizes an adjacency matrix\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    A += sp.identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D = sp.diags(inv_degs)\n",
    "    A_normalized = D.dot(A)\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    )\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55ecfc7-9ea9-4a15-83e5-18128cabacee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:06:06.225872Z",
     "iopub.status.busy": "2023-01-22T21:06:06.225081Z",
     "iopub.status.idle": "2023-01-22T21:06:30.489193Z",
     "shell.execute_reply": "2023-01-22T21:06:30.488509Z",
     "shell.execute_reply.started": "2023-01-22T21:06:06.225845Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "adj, features, edge_features = load_data()\n",
    "\n",
    "# Normalize adjacency matrices\n",
    "# adj = [normalize_adjacency(A) for A in adj]\n",
    "\n",
    "# Split data into training and test sets\n",
    "adj_train = list()\n",
    "features_train = list()\n",
    "edge_features_train = list()\n",
    "y_train = list()\n",
    "adj_test = list()\n",
    "features_test = list()\n",
    "edge_features_test = list()\n",
    "proteins_test = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03e358d-2664-4d0f-9450-71e561e6bf4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:06:30.490831Z",
     "iopub.status.busy": "2023-01-22T21:06:30.490356Z",
     "iopub.status.idle": "2023-01-22T21:06:30.509895Z",
     "shell.execute_reply": "2023-01-22T21:06:30.509272Z",
     "shell.execute_reply.started": "2023-01-22T21:06:30.490809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path + \"graph_labels.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        t = line.split(\",\")\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            adj_test.append(adj[i])\n",
    "            features_test.append(features[i])\n",
    "            edge_features_test.append(edge_features[i])\n",
    "        else:\n",
    "            adj_train.append(adj[i])\n",
    "            features_train.append(features[i])\n",
    "            edge_features_train.append(edge_features[i])\n",
    "            y_train.append(int(t[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b379b1a-0af4-475a-86fa-a226514fc21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:06:31.858563Z",
     "iopub.status.busy": "2023-01-22T21:06:31.858008Z",
     "iopub.status.idle": "2023-01-22T21:07:20.731167Z",
     "shell.execute_reply": "2023-01-22T21:07:20.730419Z",
     "shell.execute_reply.started": "2023-01-22T21:06:31.858540Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib as joblib\n",
    "\n",
    "features_train = joblib.load(\"new_features_150m_params.sav\")\n",
    "features_test = joblib.load(\"new_features_test_150m_params.sav\")\n",
    "n_input = features_train[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865db7f2-e812-434c-9a9a-b09cd4768814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:07:22.329709Z",
     "iopub.status.busy": "2023-01-22T21:07:22.329248Z",
     "iopub.status.idle": "2023-01-22T21:07:22.333503Z",
     "shell.execute_reply": "2023-01-22T21:07:22.332816Z",
     "shell.execute_reply.started": "2023-01-22T21:07:22.329685Z"
    }
   },
   "outputs": [],
   "source": [
    "# from torch_geometric.data import Data\n",
    "\n",
    "# features_train = [Data(x=torch.tensor(features_train[i]).float(), edge_index= torch.tensor(adj_train[i].todense()).nonzero().t().contiguous()) for i in range(len(features_train)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dac71149-8aa1-4fab-b715-bdd27be9ee9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:07:22.984367Z",
     "iopub.status.busy": "2023-01-22T21:07:22.983482Z",
     "iopub.status.idle": "2023-01-22T21:07:22.989791Z",
     "shell.execute_reply": "2023-01-22T21:07:22.989068Z",
     "shell.execute_reply.started": "2023-01-22T21:07:22.984340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185, 726)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65625187-f4aa-4a8e-83af-ec2441062d52",
   "metadata": {},
   "source": [
    "we have 726 vector embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b34880-1507-4f1e-9695-80e14652d2ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:07:25.884077Z",
     "iopub.status.busy": "2023-01-22T21:07:25.883417Z",
     "iopub.status.idle": "2023-01-22T21:07:30.477376Z",
     "shell.execute_reply": "2023-01-22T21:07:30.476419Z",
     "shell.execute_reply.started": "2023-01-22T21:07:25.884052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def get_Data(adj_train, features_train, edge_features_train, y_train):\n",
    "    data = []\n",
    "    for i in range(len(features_train)):\n",
    "        adj_t = torch.tensor(adj_train[i].todense())\n",
    "        edge_index = adj_t.nonzero().t().contiguous()\n",
    "        x = torch.tensor(features_train[i]).float()\n",
    "        edge_attr = torch.tensor(edge_features_train[i]).float()\n",
    "\n",
    "        data.append(\n",
    "            Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index,\n",
    "                # edge_attr=edge_attr,\n",
    "                y=y_train[i],\n",
    "            )\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "features_train = get_Data(adj_train, features_train, edge_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "794e1c52-cc8a-476b-a838-216df7e94166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:07:30.478892Z",
     "iopub.status.busy": "2023-01-22T21:07:30.478589Z",
     "iopub.status.idle": "2023-01-22T21:07:31.297433Z",
     "shell.execute_reply": "2023-01-22T21:07:31.296731Z",
     "shell.execute_reply.started": "2023-01-22T21:07:30.478873Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_Data_pred(adj_train, features_train, edge_features_train):\n",
    "    data = []\n",
    "    for i in range(len(features_train)):\n",
    "        adj_t = torch.tensor(adj_train[i].todense())\n",
    "        edge_index = adj_t.nonzero().t().contiguous()\n",
    "        x = torch.tensor(features_train[i]).float()\n",
    "        edge_attr = torch.tensor(edge_features_train[i]).float()\n",
    "\n",
    "        data.append(\n",
    "            Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index,\n",
    "                # edge_attr=edge_attr\n",
    "            )\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "features_test = get_Data_pred(adj_test, features_test, edge_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae53b548-e3e9-4d1a-b8c0-934ac088886a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:07:32.042623Z",
     "iopub.status.busy": "2023-01-22T21:07:32.042303Z",
     "iopub.status.idle": "2023-01-22T21:07:32.051109Z",
     "shell.execute_reply": "2023-01-22T21:07:32.050440Z",
     "shell.execute_reply.started": "2023-01-22T21:07:32.042600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 4399\n",
      "Number of test graphs: 489\n"
     ]
    }
   ],
   "source": [
    "train_dataset = features_train[: int(len(features_train) * 9 / 10)]\n",
    "test_dataset = features_train[int(len(features_train) * 9 / 10) :]\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of test graphs: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ba173eb-176a-4f18-9a29-e33c980ac76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:07:34.437843Z",
     "iopub.status.busy": "2023-01-22T21:07:34.437563Z",
     "iopub.status.idle": "2023-01-22T21:07:34.444979Z",
     "shell.execute_reply": "2023-01-22T21:07:34.444277Z",
     "shell.execute_reply.started": "2023-01-22T21:07:34.437822Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbe0be-36d4-47c3-9295-6c152c445943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show 3 first batches\n",
    "for step, data in enumerate(train_loader):\n",
    "    if step < 3:\n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(\"=======\")\n",
    "        print(f\"Number of graphs in the current batch: {data.num_graphs}\")\n",
    "        print(data)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9d7e6-66a9-4808-aef3-35f8d6b2863f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03dd6a35-7b3e-4a5a-a935-e8862ca4ef08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:07:38.189871Z",
     "iopub.status.busy": "2023-01-22T21:07:38.189581Z",
     "iopub.status.idle": "2023-01-22T21:07:39.084571Z",
     "shell.execute_reply": "2023-01-22T21:07:39.083858Z",
     "shell.execute_reply.started": "2023-01-22T21:07:38.189859Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (layer1): GCNConv(726, 64)\n",
      "  (layer2): GCNConv(64, 32)\n",
      "  (decoder): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, global_add_pool, SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "num_classes = 18\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"GCN\",\n",
    "        hidden_channels=[64, 64, 32],\n",
    "        num_heads=[1, 1, 1],\n",
    "        dropout=0.02,\n",
    "        n_classes=num_classes,\n",
    "        input_dim=n_input,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.hidden_dim = hidden_channels[2]\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if model_name == \"GCN\":\n",
    "            self.layer1 = GCNConv(\n",
    "                in_channels=input_dim, out_channels=hidden_channels[1]\n",
    "            )\n",
    "            self.layer2 = GCNConv(\n",
    "                in_channels=hidden_channels[1], out_channels=hidden_channels[2]\n",
    "            )\n",
    "            # self.layer3 = GCNConv(\n",
    "            #     in_channels=hidden_channels[1], out_channels=hidden_channels[2]\n",
    "            # )\n",
    "\n",
    "        elif model_name == \"GAT\":\n",
    "            self.layer1 = GATConv(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=hidden_channels[0],\n",
    "                heads=num_heads[0],\n",
    "                edge_dim=5,\n",
    "            )\n",
    "            self.layer2 = GATConv(\n",
    "                hidden_channels[0] * num_heads[0],\n",
    "                hidden_channels[1],\n",
    "                heads=num_heads[1],\n",
    "                edge_dim=5,\n",
    "            )\n",
    "            self.layer3 = GATConv(\n",
    "                hidden_channels[1] * num_heads[1],\n",
    "                hidden_channels[2],\n",
    "                heads=1,\n",
    "                edge_dim=5,\n",
    "                concat=False,\n",
    "            )\n",
    "\n",
    "        elif model_name == \"GraphSAGE\":\n",
    "            self.layer1 = SAGEConv(input_dim, hidden_channels[0], aggr=\"lstm\")\n",
    "            self.layer2 = SAGEConv(hidden_channels[0], hidden_channels[1], aggr=\"lstm\")\n",
    "            self.layer3 = SAGEConv(hidden_channels[1], hidden_channels[2], aggr=\"lstm\")\n",
    "\n",
    "        self.decoder = nn.Linear(hidden_channels[2], n_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        if self.model_name == \"GAT\":\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.layer1(x, edge_index, edge_attr)\n",
    "            x = F.elu(x)\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.layer2(x, edge_index, edge_attr)\n",
    "            x = F.elu(x)\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.layer3(x, edge_index, edge_attr)\n",
    "        else:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.layer1(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            x = self.layer2(x, edge_index)\n",
    "\n",
    "        x = F.elu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        # return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "model = GNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6212a50-6ae4-4edf-82b4-caa7770e4b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T21:16:20.079707Z",
     "iopub.status.busy": "2023-01-22T21:16:20.079424Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.8721 acc_train: 73.9941 loss_val: 0.8529\n",
      "Validation Loss Decreased(inf--->0.852892) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 002 loss_train: 0.8704 acc_train: 74.1759 loss_val: 0.8553\n",
      "Validation loss increased :(\n",
      "Epoch: 003 loss_train: 0.8667 acc_train: 74.1759 loss_val: 0.8369\n",
      "Validation Loss Decreased(0.852892--->0.836949) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 004 loss_train: 0.8643 acc_train: 73.5394 loss_val: 0.8373\n",
      "Validation loss increased :(\n",
      "Epoch: 005 loss_train: 0.8626 acc_train: 74.2441 loss_val: 0.8334\n",
      "Validation Loss Decreased(0.836949--->0.833382) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 006 loss_train: 0.8619 acc_train: 74.2214 loss_val: 0.8242\n",
      "Validation Loss Decreased(0.833382--->0.824189) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 007 loss_train: 0.8511 acc_train: 74.9716 loss_val: 0.8193\n",
      "Validation Loss Decreased(0.824189--->0.819267) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 008 loss_train: 0.8479 acc_train: 74.9943 loss_val: 0.8260\n",
      "Validation loss increased :(\n",
      "Epoch: 009 loss_train: 0.8538 acc_train: 74.5851 loss_val: 0.8386\n",
      "Validation loss increased :(\n",
      "Epoch: 010 loss_train: 0.8467 acc_train: 74.7897 loss_val: 0.8233\n",
      "Validation loss increased :(\n",
      "Epoch: 011 loss_train: 0.8431 acc_train: 74.9034 loss_val: 0.8208\n",
      "Validation loss increased :(\n",
      "Epoch: 012 loss_train: 0.8442 acc_train: 74.4487 loss_val: 0.8255\n",
      "Validation loss increased :(\n",
      "Epoch: 013 loss_train: 0.8364 acc_train: 75.2898 loss_val: 0.8323\n",
      "Validation loss increased :(\n",
      "Epoch: 014 loss_train: 0.8378 acc_train: 75.1762 loss_val: 0.8055\n",
      "Validation Loss Decreased(0.819267--->0.805466) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 015 loss_train: 0.8270 acc_train: 75.1080 loss_val: 0.7959\n",
      "Validation Loss Decreased(0.805466--->0.795902) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 016 loss_train: 0.8292 acc_train: 74.7670 loss_val: 0.7918\n",
      "Validation Loss Decreased(0.795902--->0.791836) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 017 loss_train: 0.8210 acc_train: 75.9263 loss_val: 0.7963\n",
      "Validation loss increased :(\n",
      "Epoch: 018 loss_train: 0.8202 acc_train: 75.5172 loss_val: 0.7969\n",
      "Validation loss increased :(\n",
      "Epoch: 019 loss_train: 0.8177 acc_train: 75.9718 loss_val: 0.8141\n",
      "Validation loss increased :(\n",
      "Epoch: 020 loss_train: 0.8099 acc_train: 75.9263 loss_val: 0.8046\n",
      "Validation loss increased :(\n",
      "Epoch: 021 loss_train: 0.8084 acc_train: 75.8581 loss_val: 0.7882\n",
      "Validation Loss Decreased(0.791836--->0.788186) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 022 loss_train: 0.8089 acc_train: 75.5172 loss_val: 0.8046\n",
      "Validation loss increased :(\n",
      "Epoch: 023 loss_train: 0.8081 acc_train: 75.8354 loss_val: 0.7669\n",
      "Validation Loss Decreased(0.788186--->0.766903) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 024 loss_train: 0.8012 acc_train: 76.0855 loss_val: 0.7882\n",
      "Validation loss increased :(\n",
      "Epoch: 025 loss_train: 0.8017 acc_train: 76.1082 loss_val: 0.7854\n",
      "Validation loss increased :(\n",
      "Epoch: 026 loss_train: 0.7942 acc_train: 75.7900 loss_val: 0.8064\n",
      "Validation loss increased :(\n",
      "Epoch: 027 loss_train: 0.7977 acc_train: 75.7218 loss_val: 0.7812\n",
      "Validation loss increased :(\n",
      "Epoch: 028 loss_train: 0.7971 acc_train: 76.4037 loss_val: 0.7889\n",
      "Validation loss increased :(\n",
      "Epoch: 029 loss_train: 0.7912 acc_train: 76.2219 loss_val: 0.7563\n",
      "Validation Loss Decreased(0.766903--->0.756338) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 030 loss_train: 0.7869 acc_train: 76.6083 loss_val: 0.7646\n",
      "Validation loss increased :(\n",
      "Epoch: 031 loss_train: 0.7782 acc_train: 76.9038 loss_val: 0.7619\n",
      "Validation loss increased :(\n",
      "Epoch: 032 loss_train: 0.7805 acc_train: 76.3128 loss_val: 0.7625\n",
      "Validation loss increased :(\n",
      "Epoch: 033 loss_train: 0.7782 acc_train: 76.7674 loss_val: 0.7576\n",
      "Validation loss increased :(\n",
      "Epoch: 034 loss_train: 0.7791 acc_train: 76.2219 loss_val: 0.8056\n",
      "Validation loss increased :(\n",
      "Epoch: 035 loss_train: 0.7691 acc_train: 77.1766 loss_val: 0.7571\n",
      "Validation loss increased :(\n",
      "Epoch: 036 loss_train: 0.7733 acc_train: 76.8811 loss_val: 0.7618\n",
      "Validation loss increased :(\n",
      "Epoch: 037 loss_train: 0.7684 acc_train: 76.9493 loss_val: 0.7392\n",
      "Validation Loss Decreased(0.756338--->0.739165) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 038 loss_train: 0.7609 acc_train: 76.9038 loss_val: 0.7470\n",
      "Validation loss increased :(\n",
      "Epoch: 039 loss_train: 0.7689 acc_train: 77.2221 loss_val: 0.7225\n",
      "Validation Loss Decreased(0.739165--->0.722469) \t Saving The Model\n",
      "\n",
      "\n",
      "Epoch: 040 loss_train: 0.7569 acc_train: 77.0175 loss_val: 0.7390\n",
      "Validation loss increased :(\n",
      "Epoch: 041 loss_train: 0.7609 acc_train: 77.3812 loss_val: 0.7561\n",
      "Validation loss increased :(\n"
     ]
    }
   ],
   "source": [
    "# model = GCN(hidden_channels=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4) #weight_decay=1e-4\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    loss_train = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(\n",
    "        train_loader\n",
    "    ):  # Iterate in batches over the training dataset.\n",
    "        data.to(device)\n",
    "        out = model(\n",
    "            data.x, data.edge_index, _, data.batch\n",
    "        )  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        # test part\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        loss_train += criterion(out, data.y)\n",
    "\n",
    "\n",
    "    return correct / len(train_loader.dataset) * 100, loss_train / (i + 1)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(\n",
    "        loader\n",
    "    ):  # Iterate in batches over the training/test dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index,_, data.batch)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        loss += criterion(out, data.y)\n",
    "        \n",
    "        \n",
    "    # torch.cuda.empty_cache()\n",
    "    return correct / len(loader.dataset) * 100, loss / (\n",
    "        i + 1\n",
    "    )  # Derive ratio of correct predictions. , proba\n",
    "\n",
    "\n",
    "min_val_loss = float(\"inf\")\n",
    "for epoch in range(1, 401):\n",
    "    train_acc, train_loss = train()\n",
    "    test_acc, test_loss = test(train_loader)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        # test_loss = test(test_loader)[1].to(\"cpu\").detach().numpy()\n",
    "        print(\n",
    "            \"Epoch: {:03d}\".format(epoch),\n",
    "            \"loss_train: {:.4f}\".format(train_loss),\n",
    "            \"acc_train: {:.4f}\".format(train_acc),\n",
    "            \"loss_val: {:.4f}\".format(test_loss),\n",
    "            # \"acc_val: {:.4f}\".format(test_acc),\n",
    "        )\n",
    "        # print(test(test_loader))\n",
    "        if min_val_loss > test_loss:\n",
    "            print(\n",
    "                f\"Validation Loss Decreased({min_val_loss:.6f}--->{test_loss:.6f}) \\t Saving The Model\"\n",
    "            )\n",
    "            min_val_loss = test_loss\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), \"saved_model_150m_params_GAT.pth\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"Validation loss increased :(\")\n",
    "        # print(f\"Epoch: {epoch:03d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73bf9280-d04e-4ce3-8e3b-f4d4198809b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-21T15:53:07.716742Z",
     "iopub.status.busy": "2023-01-21T15:53:07.716123Z",
     "iopub.status.idle": "2023-01-21T15:53:07.720982Z",
     "shell.execute_reply": "2023-01-21T15:53:07.720480Z",
     "shell.execute_reply.started": "2023-01-21T15:53:07.716721Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81f7788a-8c94-497c-a68f-8dfd90a8ad0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-21T22:34:58.644410Z",
     "iopub.status.busy": "2023-01-21T22:34:58.644142Z",
     "iopub.status.idle": "2023-01-21T22:34:58.655416Z",
     "shell.execute_reply": "2023-01-21T22:34:58.654834Z",
     "shell.execute_reply.started": "2023-01-21T22:34:58.644392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (layer1): GCNConv(726, 64)\n",
       "  (layer2): GCNConv(64, 32)\n",
       "  (decoder): Linear(in_features=32, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"saved_model_150m_params_GAT.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "800be3ae-319f-4e05-9edb-833c85970b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-21T22:35:04.795893Z",
     "iopub.status.busy": "2023-01-21T22:35:04.795635Z",
     "iopub.status.idle": "2023-01-21T22:35:04.808143Z",
     "shell.execute_reply": "2023-01-21T22:35:04.807521Z",
     "shell.execute_reply.started": "2023-01-21T22:35:04.795875Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "def predict(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    pred = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, _, data.batch)\n",
    "        out = softmax(out)\n",
    "        pred.append(out.to(\"cpu\").numpy())\n",
    "        # pred.append(out.argmax(dim=1))  # Use the class with highest probability.\n",
    "\n",
    "    return np.concatenate(pred, axis=0)  # Derive ratio of correct predictions. , proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9c2562d-305c-422f-95a5-44fcc4e6eb5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-21T22:35:06.963396Z",
     "iopub.status.busy": "2023-01-21T22:35:06.963131Z",
     "iopub.status.idle": "2023-01-21T22:35:07.873371Z",
     "shell.execute_reply": "2023-01-21T22:35:07.872540Z",
     "shell.execute_reply.started": "2023-01-21T22:35:06.963378Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = predict(pred_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68761999-08ae-4dab-8747-a82dca5ffb0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-21T22:35:34.249484Z",
     "iopub.status.busy": "2023-01-21T22:35:34.248979Z",
     "iopub.status.idle": "2023-01-21T22:35:34.261432Z",
     "shell.execute_reply": "2023-01-21T22:35:34.260904Z",
     "shell.execute_reply.started": "2023-01-21T22:35:34.249463Z"
    }
   },
   "outputs": [],
   "source": [
    "def submit(y_pred_proba):\n",
    "    # Write predictions to a file\n",
    "    with open(\n",
    "        \"../Submissions/geometric_GCNcov_150_param__1.17_loss.csv\", \"w\"\n",
    "    ) as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        lst = list()\n",
    "        for i in range(18):\n",
    "            lst.append(\"class\" + str(i))\n",
    "        lst.insert(0, \"name\")\n",
    "        writer.writerow(lst)\n",
    "        for i, protein in enumerate(proteins_test):\n",
    "            lst = y_pred_proba[i, :].tolist()\n",
    "            lst.insert(0, protein)\n",
    "            writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f567b-2a08-436c-9f79-71f195effcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2b56a2aa-9d94-4880-9ada-91778ebb889d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526ddf0-c6d6-4da4-b908-082208350f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
